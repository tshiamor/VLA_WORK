# VLA Pretraining Configuration
# ==============================

# Data
data:
  train_dir: "data/demos/train"
  val_dir: "data/demos/val"
  chunk_size: 16
  image_size: [224, 224]
  augment: true

# Model
model:
  vlm_model: "Qwen/Qwen2.5-VL-7B-Instruct"
  action_dim: 7
  chunk_size: 16
  hidden_dim: 512
  freeze_vlm: true
  use_lora: true
  lora_r: 16

# Training
training:
  epochs: 100
  batch_size: 32
  gradient_accumulation_steps: 4
  effective_batch_size: 128  # batch_size * gradient_accumulation_steps

  # Optimizer
  optimizer: "adamw"
  learning_rate: 1.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Scheduler
  scheduler: "cosine"
  warmup_steps: 1000
  min_lr: 1.0e-6

  # Mixed precision
  use_amp: true
  amp_dtype: "bfloat16"

# Checkpointing
checkpointing:
  checkpoint_dir: "checkpoints/pretrain"
  save_every_n_steps: 1000
  eval_every_n_steps: 500
  log_every_n_steps: 10

# Logging
logging:
  wandb_project: "vla-pretrain"
  wandb_run_name: null  # Auto-generated if null
  log_images: true

# Early stopping
early_stopping:
  patience: 10
  metric: "val_loss"

# Resources
device: "cuda"
num_workers: 4
seed: 42
