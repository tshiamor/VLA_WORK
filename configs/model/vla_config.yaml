# Complete VLA Model Configuration
# =================================

# Vision-Language Model
vision_encoder:
  model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  freeze_backbone: true
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  torch_dtype: "bfloat16"
  device_map: "auto"

# Projector (VLM -> Action Space)
projector:
  vlm_hidden_size: 3584
  action_hidden_size: 512
  num_layers: 2
  dropout: 0.1
  activation: "gelu"
  use_layer_norm: true

# Action Expert (Transformer)
action_expert:
  hidden_dim: 512
  num_layers: 6
  num_heads: 8
  mlp_ratio: 4.0
  dropout: 0.1
  attention_dropout: 0.1
  max_seq_len: 256
  proprio_dim: 14  # 7 positions + 7 velocities
  use_proprio: true

# Flow Matching Head
flow_matching:
  action_dim: 7
  hidden_dim: 512
  chunk_size: 16
  num_steps: 100
  ode_solver: "euler"

# Global settings
action_dim: 7
chunk_size: 16
use_cross_attention_projector: false
use_lightweight_action_expert: false

# Action normalization (computed from data)
action_mean: null  # Will be set from training data
action_std: null
